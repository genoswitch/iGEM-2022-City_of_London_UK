\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}

\title{Modelling Temperature Dissipation in the Mask Lysis Unit}
\date{April 2022}

\begin{document}

\maketitle

\begin{abstract}
    We aim to model the effects of heating on different conditions in the lysis unit to determine whether heating is uniform and fast enough, and how scale affects it. We use thermal iterative methods on a discrete space in three dimensions. Code is written in Python with the NumPy library, Raster Geometry and Matplotlib. Electronics information in liaison with Guantanamo Krishna. Thanks to Sophie Elena - co-writer of the code and modelling expert - and Jian-Hui Mo.
\end{abstract}

\section{Equations}
For a continuous scalar field $T:\mathbb{R}^3 \to \mathbb{R}$ representing temperature in a three dimensional volume,  its evolution is governed by the heat equation:

\begin{equation}
    \frac{\partial T}{\partial t}=\alpha \nabla^2 T
\end{equation}
... where $\alpha:\mathbb{R}^3 \to \mathbb{R}$ is the thermal diffusivity (as a scalar field).

Our model also involves a heating element, modelled as a resistor, which requires more thought. The power $P=\frac{V^2}{R}$ can be substituted into the differential form of the specific heat equation.

\begin{equation}
    dE = mc\cdot dT
\end{equation}

\begin{equation}
    dT = \frac{dE}{dt}\frac{dt}{mc}= dt \frac{P}{mc}
\end{equation}

However, the resistance of the element changes with temperature, modelled by...
\begin{equation}
    R = R_{ref}[1+\alpha(T-T_{ref})]
\end{equation}

where $R_{ref}$ and $T_{ref}$ are the resistance of wire and the temperature at which it was measured. Combining this gives, for the wire:

\begin{equation}
    dT = \frac{V^2}{mc}\frac{1}{R_{ref}[1+\alpha(T-T_{ref})]}dt
\end{equation}

This is - in accordance with the superposition theorem - added to the differential from the heat equation (which applies globally)\footnote{This is an abuse of notation, $dT$ is the total differential, where it should be $\partial t$, but these are converted to finite deltas for iteration.}:

\begin{equation}
    dT = \alpha \nabla^2T\cdot dt
\end{equation}

\section{Discretisation}
The modelled volume must be divided up into discrete voxels to implement an iterated approximation. Each voxel is assigned an array index $i,j,k: i,j,k\geq 0: i,j,k \in \mathbb{Z}$. The volume modelled is a cuboid formed of uniform cubes of size $\langle \Delta x,\Delta x,\Delta x\rangle$. The voxels are identically sized in the $z$, $x$ and $y$ dimensions, so the equations below refer only to $i$ and $x$ but apply to all.

\begin{equation}
    x = i\Delta x
\end{equation}

\begin{equation}
    i = \left\lfloor \frac{x}{\Delta x} \right\rceil
\end{equation}

\begin{equation}
    \Delta x = \frac{L}{s}
\end{equation}

Here $s$ is the number of spatial samples and $\lfloor x \rceil$ denotes the rounding of $x$ to an integer. The temperature and thermal diffusivity fields have now also become arrays, I will represent the entry at voxel $i,j,k$ as $T_{i,j,k}$ and $\alpha_{i,j,k}$, respectively. Next, the Laplacian $\nabla^2$, usually defined as an operator on continuous fields in 3D Cartesian coordinates as

\begin{equation}
    \nabla^2 = \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}
\end{equation}

must now be defined discretely (central differencing). In one dimension:

\begin{equation}
\begin{split}
    \nabla^2T_{i} = \frac{\Delta}{\Delta x}\left(\frac{\Delta T_{i}}{\Delta x}\right) = \frac{\Delta}{\Delta x}\left(\frac{T_{i+1}-T_{i}}{\Delta x}\right) \\ = \frac{T_{i+1}-T_{i}-(T_{i}-T_{i-1})}{\Delta x^2} = \frac{T_{i+1}+T_{i-1}-2T_{i}}{\Delta x^2}
\end{split}
\end{equation}
And in three\footnote{$\Delta x = \Delta y = \Delta z$}...
\begin{equation}
    (\nabla^2T)_{i,j,k} = \frac{1}{\Delta x^2}[T_{i+1,j,k}+T_{i-1,j,k}+T_{i,j+1,k}+T_{i,j-1,k}+T_{i,j,k+1}+T_{i,j,k-1}-6T_{i,j,k}]
\end{equation}

The iteration equations become (global, then wire-only extra clause):

\begin{equation}
    (\Delta T)_{i,j,k} = \alpha_{i,j,k}(\nabla^2T)_{i,j,k}\cdot\Delta t
\end{equation}

\begin{equation}
    (\Delta T)_{i,j,k} = \left[\alpha_{i,j,k}(\nabla^2T)_{i,j,k} + \frac{V^2}{mc}\frac{1}{R_{ref}[1+\alpha_{i,j,k}(T_{i,j,k}-T_{ref})]}\right]\Delta t
\end{equation}

\section{Model Parameters}
\emph{DeathStar.png [Caption: "The lysis unit can be modelled as the intersection of three regions: a large outer cylinder of PLA, a smaller inner cylinder of fluid (water), and a smallest resistive cylinder with a perpendicular axis."]}

The lysis unit can be modelled as being contained within a small simulated volume (padding the lysis unit volume by a few voxels), which itself is exposed to an infinitely absorbing environment of room temperature (this is included in the boundary conditions, see Section 7).

\section{Code}
\emph{simulation.py, matrices.py, lysissim.py}
\section{The Oscillating Grid Problem}

When running iterative models, it is more time efficient to use larger $\Delta t$. Considering the taxing array operations and high spatial sampling rate, optimising with larger time intervals was largely necessary. However, on running simulation with large enough a value of $\Delta t$, we encounter unexpected behaviour of the model.

\emph{checkerboard.gif}

Some voxels begin to oscillate, forming a checkerboard pattern which is undesired. It is evident now that this is a result of exceedingly large steps in temperature $\Delta T$. Since $\Delta T$ is directly (locally) proportional to $\Delta t$, there is a limit to how large the time interval can be, before it begins to induce the checkerboard oscillation observed. Knowing this limit would give us the best value for $\Delta t$.

Finding this limit starts with a model of the model. Consider an infinite grid of alternating pixels in 2 dimensions, with temperatures $T_0$ and $T_1$ and all with uniform $\alpha$.

This problem is symmetric: all pixels with the same initial temperature behave the same way. We expect the grid to converge to a state where all the pixels have reached a mean temperature.

\begin{equation}
    T_\mu = \frac{1}{2}(T_0(0)+T_1(0))
\end{equation}

The Laplacian for pixel 0 $\nabla^2 T_0$ is:

\begin{equation}
    \nabla^2 T_0 = \frac{2}{\Delta x^2}(2T_1-2T_0)
\end{equation}

more generally in n dimensions

\begin{equation}
    \nabla^2 T_0 = \frac{2n}{\Delta x^2}(T_1-T_0)
\end{equation}

Similarly, for pixels 1:

\begin{equation}
    \nabla^2 T_1 = \frac{2n}{\Delta x^2}(T_0-T_1)
\end{equation}

Therefore, the heat equation for both pixels yields...

\begin{equation}
    \frac{dT_0}{dt} = \alpha\nabla^2 T_0 = \frac{2n\alpha}{\Delta x^2}(T_1-T_0)
\end{equation}

\begin{equation}
    \frac{dT_1}{dt} = \alpha\nabla^2 T_1 = \frac{2n\alpha}{\Delta x^2}(T_0-T_1)
\end{equation}

Equations (19) and (20) are coupled first order linear differential equations in the two temperatures, solved analytically:

\begin{equation*}
    \frac{dT_0}{dt} =-\frac{dT_1}{dt}
\end{equation*}

\begin{equation*}
    \implies T_0=-T_1+C
\end{equation*}

\begin{equation}
    C = 2T_\mu
\end{equation}

\begin{equation*}
    \frac{dT_0}{dt} =  \frac{2n\alpha}{\Delta x^2}(2T_\mu-2T_0)
\end{equation*}

\begin{equation}
    k := \frac{4n\alpha}{\Delta x^2},\quad \frac{dT_0}{dt} =  k(T_\mu-T_0)
\end{equation}

\begin{equation*}
    \int \frac{1}{T_\mu-T_0}dT_0 =  k\int dt
\end{equation*}

\begin{equation*}
    -\ln|T_\mu-T_0| =  kt+D
\end{equation*}

\begin{equation*}
    T_\mu-T_0 =  De^{-kt}
\end{equation*}

\begin{equation}
    T_0(t) =  T_\mu-De^{-kt}
\end{equation}

I reassigned the letter $D$ from the constant of integration to what would be $e^{-D}$, it doesn't matter as it acts as some constant until we find its value, which is $D=T_\mu-T_0(0)$. Solving for $T_1$ is similar...

\begin{equation}
    \frac{dT_1}{dt} =  k(T_\mu-T_1)
\end{equation}

\begin{equation*}
    T_1(t) =  T_\mu-Be^{-kt}
\end{equation*}

Here $B=T_\mu-T_1(0)=-D$

\begin{equation}
    T_1(t) =  T_\mu+De^{-kt}
\end{equation}

\emph{<iframe src="https://www.desmos.com/calculator/gjawwhqvyd?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>}

The iterative solution must be expressed in terms of a sum.

\begin{equation}
    \Delta T_0 = \frac{k}{2}(T_1-T_0)\Delta t
\end{equation}

\begin{equation}
    \Delta T_1 = \frac{k}{2}(T_0-T_1)\Delta t
\end{equation}


\begin{equation}
    T_0^{[i+1]} = T_0^{[i]}+\frac{k}{2}(T_1^{[i]}-T_0^{[i]})\Delta t = T_0^{[i]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[i]}\frac{k}{2}\Delta t 
\end{equation}

\begin{equation}
\begin{split}
    T_0^{[1]} = T_0^{[0]}+\frac{k}{2}(T_1^{[0]}-T_0^{[0]})\Delta t \\ = T_0^{[0]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[0]}\frac{k}{2}\Delta t 
\end{split}
\end{equation}

\begin{equation*}
    T_0^{[2]} =  T_0^{[1]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[1]}\frac{k}{2}\Delta t 
\end{equation*}

\begin{equation*}
    T_0^{[2]} =  \left[T_0^{[0]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[0]}\frac{k}{2}\Delta t\right]\left(1-\frac{k}{2}\Delta t\right)+\left[T_1^{[0]}\left(1-\frac{k}{2}\Delta t\right)+T_0^{[0]}\frac{k}{2}\Delta t\right]\frac{k}{2}\Delta t
\end{equation*}

\begin{equation}
    T_0^{[2]} =  T_0^{[0]}\left[\left(1-\frac{k}{2}\Delta t\right)^2+\left(\frac{k}{2}\Delta t\right)^2\right]+T_1^{[0]}\left[2\left(1-\frac{k}{2}\Delta t\right)\left(\frac{k}{2}\Delta t\right)\right]
\end{equation}

\begin{equation*}
    T_0^{[3]} =  T_0^{[2]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[2]}\frac{k}{2}\Delta t 
\end{equation*}

\begin{equation*}
\begin{split}
    T_0^{[3]} = & T_0^{[0]}\left[\left(1-\frac{k}{2}\Delta t\right)^3+\left(\frac{k}{2}\Delta t\right)^2\left(1-\frac{k}{2}\Delta t\right)+2\left(1-\frac{k}{2}\Delta t\right)\left(\frac{k}{2}\Delta t\right)^2\right] + \\ & T_1^{[0]}\left[2\left(1-\frac{k}{2}\Delta t\right)^2\left(\frac{k}{2}\Delta t\right)+\left(1-\frac{k}{2}\Delta t\right)^2\left(\frac{k}{2}\Delta t\right)+\left(\frac{k}{2}\Delta t\right)^3\right]
\end{split}
\end{equation*}

\begin{equation}
\begin{split}
    T_0^{[3]} = &  T_0^{[0]}\left[\left(1-\frac{k}{2}\Delta t\right)^3+3\left(1-\frac{k}{2}\Delta t\right)\left(\frac{k}{2}\Delta t\right)^2\right] + \\ & T_1^{[0]}\left[3\left(1-\frac{k}{2}\Delta t\right)^2\left(\frac{k}{2}\Delta t\right)+\left(\frac{k}{2}\Delta t\right)^3\right]
\end{split}
\end{equation}

Equations (29), (30) and (31) lead us to draw the conclusion that:

\begin{equation}
        T_0^{[n]} =  \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n+1}+T_\mu\right]
\end{equation}

\begin{equation}
        T_1^{[n]} =  \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n}+T_\mu\right]
\end{equation}

Proof by induction follows:

\begin{equation}
\begin{split}
        T_0^{[1]} =  \sum_{r=0}^{1}\binom{1}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{1-r}\left[D(-1)^{r+2}+T_\mu\right] \\ =  \left(\frac{k}{2}\Delta t\right)\left[D(-1)^2+T_\mu\right] + \left(1-\frac{k}{2}\Delta t\right)\left[D(-1)^3+T_\mu\right]
        \\ =  \left(\frac{k}{2}\Delta t\right)T_1^{[0]} + \left(1-\frac{k}{2}\Delta t\right)T_0^{[0]}
\end{split}
\end{equation}

\begin{equation*}
\begin{split}
        T_0^{[n+1]} = T_0^{[n]}\left(1-\frac{k}{2}\Delta t\right)+T_1^{[n]}\left(\frac{k}{2}\Delta t \right) \\  = \left(1-\frac{k}{2}\Delta t\right) \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n+1}+T_\mu\right] \\ + \left(\frac{k}{2}\Delta t\right) \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n}+T_\mu\right] \\  = \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^{r+1}\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n+1}+T_\mu\right] \\ +  \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r+1}\left[D(-1)^{
        r+n}+T_\mu\right] \\  = \sum_{r=1}^{n+1}\binom{n}{r-1}\left(1-\frac{k}{2}\Delta t\right)^{r}\left(\frac{k}{2}\Delta t\right)^{n-r+1}\left[D(-1)^{r+n}+T_\mu\right] \\ +  \sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r+1}\left[D(-1)^{r+n}+T_\mu\right] \\  = \sum_{r=1}^{n}\left[\binom{n}{r-1}+\binom{n}{r}\right]\left(1-\frac{k}{2}\Delta t\right)^{r}\left(\frac{k}{2}\Delta t\right)^{n+1-r}\left[D(-1)^{r+n}+T_\mu\right] \\ +  \binom{n}{0}\left(1-\frac{k}{2}\Delta t\right)^0\left(\frac{k}{2}\Delta t\right)^{n+1}\left[D(-1)^{n}+T_\mu\right] \\ + \binom{n}{n}\left(1-\frac{k}{2}\Delta t\right)^{n+1}\left(\frac{k}{2}\Delta t\right)^{0}\left[D(-1)^{2n+1}+T_\mu\right]
\end{split}
\end{equation*}

The binomial coefficient:

\begin{equation*}
\begin{split}
        \binom{n}{r-1}+\binom{n}{r} = \frac{n!}{(r-1)!(n-r+1)!}+\frac{n!}{r!(n-r)!} \\=
        \frac{n!}{(r-1)!(n-r)!}\left[\frac{1}{n-r+1}+\frac{1}{r}\right]
        \\=
        \frac{n!}{(r-1)!(n-r)!}\left[\frac{n+1}{(n-r+1)r}\right]
        \\=
        \frac{(n+1)!}{r!(n+1-r)!}=\binom{n+1}{r}
\end{split}
\end{equation*}

\begin{equation}
\begin{split}
        T_0^{[n+1]} = \sum_{r=1}^{n}\binom{n+1}{r}\left(1-\frac{k}{2}\Delta t\right)^{r}\left(\frac{k}{2}\Delta t\right)^{n+1-r}\left[D(-1)^{r+(n+1)+1}+T_\mu\right] \\ +  \binom{n+1}{0}\left(1-\frac{k}{2}\Delta t\right)^0\left(\frac{k}{2}\Delta t\right)^{n+1}\left[D(-1)^{(n+1)+1}+T_\mu\right] \\ + \binom{n+1}{n+1}\left(1-\frac{k}{2}\Delta t\right)^{n+1}\left(\frac{k}{2}\Delta t\right)^{0}\left[D(-1)^{n+1+(n+1)+1}+T_\mu\right] \\= \sum_{r=0}^{n+1}\binom{n+1}{r}\left(1-\frac{k}{2}\Delta t\right)^{r}\left(\frac{k}{2}\Delta t\right)^{n+1-r}\left[D(-1)^{r+(n+1)+1}+T_\mu\right]
\end{split}
\end{equation}

The algebra of $(-1)^x$ allows us to add or subtract any even number to $x$ and maintain the same value, which was exploited for the proof. Equation (32) is true for $n=1$, and if it is true for $n$, then it is also true for $n+1$. By the principle of mathematical induction, the formula must be true for all $n\in\mathbb{N}$. The formulae can be further simplified:

\begin{equation*}
\begin{split}
    T_0^{[n]}=\sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\left[D(-1)^{r+n+1}+T_\mu\right]\\ = T_\mu\sum_{r=0}^{n}\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r} + D(-1)^{n+1}\sum_{r=0}^{n}(-1)^r\binom{n}{r}\left(1-\frac{k}{2}\Delta t\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}\\ = T_\mu\left(1-\frac{k}{2}\Delta t+\frac{k}{2}\Delta t\right)^n + D(-1)^{n+1}\sum_{r=0}^{n}\binom{n}{r}\left(\frac{k}{2}\Delta t-1\right)^r\left(\frac{k}{2}\Delta t\right)^{n-r}
    \\ = T_\mu+ D(-1)^{n+1}(k\Delta t-1)^n
\end{split}
\end{equation*}

\begin{equation}
    T_0^{[n]} = T_\mu- D(1-k\Delta t)^n
\end{equation}

\begin{equation}
    T_1^{[n]} = T_\mu+ D(1-k\Delta t)^n
\end{equation}

Here are side-by-side comparisons of the analytic solutions and the iterated solutions with certain $\Delta t$ values, the resulting behaviours of which we will impose conditions on in the following section.

\emph{<iframe src="https://www.desmos.com/calculator/ejggksictr?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>}

\section{Conditions for Convergent Iteration}

We put forwards some basic conditions for convergence of the checkerboard grid: conditions to stop the oscillation that is observed with large enough $\Delta t$. The \emph{absolute condition for convergent iteration} is, using equation (26)...

\begin{equation*}
    \Delta T_0 < T_1-T_0
\end{equation*}

\begin{equation*}
    \Delta t < \frac{2}{k}\frac{T_1-T_0}{T_1-T_0}
\end{equation*}

\begin{equation}
    \Delta t < \frac{\Delta x^2}{2n\alpha_{max}}
\end{equation}

Note the use of $\alpha_{max}$ to find the global upper bound for convergence, as $\Delta t$ is a global constant. The next condition eliminates the possibility of either $\Delta T$ vacillating from negative to positive (\emph{condition for unidirectional convergence}):

\begin{equation*}
    \Delta T_0 < \frac{1}{2}(T_1-T_0)
\end{equation*}

\begin{equation}
    \Delta t < \frac{\Delta x^2}{4n\alpha_{max}}
\end{equation}

Finally we can impose a third condition in order to get \emph{meaningful} or \emph{accurate} convergent iteration. An error function between the analytic and iterated solutions allows us to choose a desired error level and return a suitable value for $\Delta t$. The absolute error between the iteration and the analytic solution at discrete intervals $t_i=i\Delta t$ as a function of the parameter $\Delta t$ is:

\begin{equation*}
    \varepsilon(\Delta t) = \sum_{i=0}^{\infty} 2\left|[T_\mu -De^{-ik\Delta t}]-[T_\mu-D(1-k\Delta t )^i]\right|
\end{equation*}

\begin{equation*}
    \varepsilon(\Delta t) = 2|D|\sum_{i=0}^{\infty} \left|e^{-ik\Delta t}-(1-k\Delta t)^i\right|
\end{equation*}

Notice due to the symmetry of the solutions, we can account for the $T_1$ error by multiplying the $T_0$ error by 2. At this point it becomes easier to convert this into an integral, for which we need to extend the iterated solution's domain from discrete to continuous, using $t=i\Delta t$. 

\begin{equation*}
    \varepsilon(\Delta t) = 2|D|\int_{0}^{\infty} \left|e^{-kt}-(1-k\Delta t)^{\frac{t}{\Delta t}} \right| dt
\end{equation*}

To remove the modulus, we have to assert that $e^{-kt}\geq (1-k\Delta t)^{\frac{t}{\Delta t}}$ for $t\geq0$. This is difficult to prove, though it seems to hold up to large $t$ for most parameters, and any time this assumption breaks at large $t$, the difference between the analytic and iterated solutions will be almost negligible, so we can safely work with this condition. The error function becomes:

\begin{equation*}
    \varepsilon(\Delta t) = 2|D|\int_{0}^{\infty} e^{-kt}-(1-k\Delta t)^{\frac{t}{\Delta t}} dt
\end{equation*}

\begin{equation*}
    = 2|D|\left[ -\frac{e^{-kt}}{k}-\frac{(1-k\Delta t)^{\frac{t}{\Delta t}}}{\ln(1-k\Delta t)}\Delta t \right]_0^\infty
\end{equation*}

\begin{equation}
    \varepsilon(\Delta t) = 2|D|\left[ \frac{1}{k}+\frac{\Delta t}{\ln(1-k\Delta t)} \right]
\end{equation}

Removing the $2|D|$ to standardise the error function for all initial parameter, this can be approximated quadratically with a Taylor expansion around $\Delta t = 0$:

\begin{equation}
    \varepsilon_{standard}(\Delta t) \approx \frac{1}{2}x+\frac{k}{12}x^2+\frac{k^2}{24}x^3 + \frac{19k^3}{720}x^4
\end{equation}

This error function allows us to crudely estimate how accurate any temperature simulation will be, given a value for $\Delta t$.

\section{Results}

After the implementation of basic boundary conditions, including 'repeat' (modular space), 'void' (infinite environment with fixed temperature) and 'adiabatic' (no heat lost to surroundings, energy conserved within volume):

\emph{for each gif: void.gif, void2.gif [Caption: "Void boundary condition"]}


...the simulation was run with the model parameters, with the convergent $\Delta t$ and rasterised using the Raster Geometry package. The results follow, with an evolving heatmap showing the temperature in a cross-section of the model volume and a graph tracking the temperature of fixed points in the volume over time.

\emph{finalheatmap.gif}

\emph{fixedpointsample.gif}


\end{document}
